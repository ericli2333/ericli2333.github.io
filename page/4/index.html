<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Blogs</title><meta name="author" content="Eric Li"><meta name="copyright" content="Eric Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Blogs">
<meta property="og:url" content="https://www.ericli.vip/page/4/index.html">
<meta property="og:site_name" content="Blogs">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.ericli.vip/img/OIP.jpg">
<meta property="article:author" content="Eric Li">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.ericli.vip/img/OIP.jpg"><link rel="shortcut icon" href="/img/OIP.jpg"><link rel="canonical" href="https://www.ericli.vip/page/4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Blogs',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-03-07 19:32:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Blogs" type="application/atom+xml">
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/OIP.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">80</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">32</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/fufu.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Blogs"><span class="site-name">Blogs</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Blogs</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/ericli2333" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/21/RL/Learning%20Notes/Tmux%20usage/" title="Tmux 使用简介">Tmux 使用简介</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-21T11:59:53.000Z" title="Created 2024-03-21 19:59:53">2024-03-21</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%95%99%E7%A8%8B/">教程</a></span></div><div class="content">tmux简介tmux是链接服务器跑服务的神器，可以在取消链接之后继续运行想要运行的程序
使用流程安装tmux使用
1sudo apt install tmux
即可
新建窗口1tmux new -s NAME
即可创建一个名为name的session，然后在里面运行你的指令即可
然后就可以直接关掉这个链接了
退出窗口如果想要退出当前的tmux session 可以先按下 ctrl + B 然后松开（这个时候没有变化是正常的）然后按下 D 就可以在不终止当前任务的情况下退出了。如果想直接终止这个任务，可以按下 ctrl + B + D 即不松手就行了。 
关闭session使用命令
1tmux ls
查看当前在运行的session，使用
1tmux kill-sesion -t NAME
关掉session就可以了
恢复session使用命令
1tmux a -t NAME
可以恢复一个session
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/20/RL/Learning%20Notes/Environment%20Configuration/" title="Gymnasium Environment Configuration">Gymnasium Environment Configuration</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-20T09:07:42.000Z" title="Created 2024-03-20 17:07:42">2024-03-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">强化学习环境——gymnasium配置注意，现在已经是2024年了，建议使用最新的gymnasium而不是gym
配置正确的python版本现在是2024年的3月20日，目前的gymnasium不支持python3.12，建议使用conda创建一个3.11的环境：
1conda create -n RL python=3.11
然后进入这个环境中：
1conda activate RL
如果使用的是Windows下的powershell，此时你的终端最前面没有显示例如：
1(RL) xxx@xxx.xxx.xxx.xxx:~

而是：
1xxx@xxx.xxx.xxx.xxx:~

的话，建议先运行：
1conda init 
然后使用
1conda info
查看一下现在的环境是不是激活成功了
安装gymnasium这里有两个坑，第一个是直接安装 gymnasium 只是装了个白板，里面啥也没有，需要安装的是 gymnasium[atari] 和 gymnasium[accept-rom-license]记住，两个都要装
第二个坑是不知道为什么用conda install没有效果，所 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/18/RL/Article%20Reading/DQN/" title="DQN">DQN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-18T14:25:06.000Z" title="Created 2024-03-18 22:25:06">2024-03-18</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/Article-Reading/">Article_Reading</a></span></div><div class="content">Article Reading本次DQN我选择阅读的是1312.5602这篇论文
Motivation
过往的方法在处理高维度的输入，例如视频和音频的时候略显乏力，往往依赖人工选取特征，但是可以通过卷积神经网络、多层感知机等方式直接利用神经网络来提取高维的特征。
深度学习里面往往需要大量的有标签的样本，但是RL有延迟奖励问题，一个动作的价值可能需要一个Episode结束之后才能确定
深度学习里面有一个重要假设是独立同分布采样，但是RL里面的数据往往是有很高的相关性的，不符合该假设

Idea如何处理图像输入前面套一个CNN对图像进行卷积处理，提取图像的特征进行有效的降维处理
如何处理样本量少的问题使用经验回放数组的方法，即创建一个buffer，每次获得一个状态-动作-价值-下一状态组的时候，不仅是运用这个组来进行训练，更是把它放到buffer里面，每次训练的时候从里面采样出一个batch，利用batch来进行训练
Limitation单神经网络训练不稳定在13年的原版论文里面，使用的是一个神经网络，没有分为target网络和Q网络，导致在训练的时候loss上下波动比较大，reward上 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/13/RL/RLBook2020%20Learning/Policy%20Gradient%20Methods/" title="Policy Gradient Methods">Policy Gradient Methods</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-13T08:43:39.000Z" title="Created 2024-03-13 16:43:39">2024-03-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Background本书前面的部分主要讲的都是学习价值函数的方法，这里提出一种直接学习策略的方法，这里把策略记作一个带有参数的，即  
Advantage优点之一是可以学习一个确定性的算法而不像  - greedy 的策略那样每次都有一个较小的概率选择非最优解。同时，基于价值函数学习的方法里面如何选择初始值和如何进行递降都是需要考虑的问题。
Policy Gradient在直接学习策略的时候，正确地更新参数  是十分重要的，所以需要想办法求出评估量对于  的梯度，此处定义：对于分幕式任务，在经过推导（RLBook2020 P325）后，得到：

这里虽然只找出了正比关系，但是在梯度下降的时候，只关注梯度的方向，并不关心梯度真正的值是多少

Monte Carlo Policy Gradient根据上面的式子，写成期望的形式：那么就可以得出梯度下降的公式：其中  是对于真实动作价值函数的逼近。上面的公式成为 all-actions methods 因为它包含了该状态所有可能的动作  下面介绍另一种强化学习版本的。
这里的  是依据策略  在  时刻采样出一个动作，第一个等号相等的原因是因 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/12/RL/RLBook2020%20Learning/On-policy%20Control%20with%20Approximation/" title="On-policy Control withApproximation">On-policy Control withApproximation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-12T13:06:40.000Z" title="Created 2024-03-12 21:06:40">2024-03-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Episodic Semi-gradient Control这里和上一章的公式的区别只是把状态价值函数改成了动作价值函数，即：对于一步的Sarsa算法来说，上面的公式应该写为：其算法流程图如下：而对于  步的Sarsa来说，和前面也没有太大的差别：只是这里的价值函数更新的时候变成了以n为周期的。
平均回报对于可以分为一个个Episode的任务，前面的折后回报是可以处理的，但是对于连续型任务是不够的，定义一个策略  的平均回报  如下：那么这个时候的状态价值函数、动作价值函数和最优状态价值函数和最优动作价值函数都可以写成一个新的形式：最优的就是取最大值就行了。

注意到上面的式子里面没有折扣率  了，因为在连续型任务中，先后出现的价值在重要性上没有区别。

在这种定义下的单步Sarsa算法如下：n步Sarsa的算法如下图所示：
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/10/RL/RLBook2020%20Learning/On-policy%20Prediction%20with%20Approximation/" title="On-policy Prediction with Approximation">On-policy Prediction with Approximation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-10T07:53:49.000Z" title="Created 2024-03-10 15:53:49">2024-03-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">提出背景由于某些问题的空间维度可能会很高，直接使用tabular的方法来保存所有信息是不现实的，所以考虑换一种方法来表示价值函数，即使用  来近似替代原来的状态价值函数
均方误差为了评估近似替代版本的价值函数和原始的价值函数之间的距离，这里提出均方误差  其定义为：
这其中的  是状态的分布，是状态  出现的概率
SGD和Semi-gradient MethodsSGD 的更新公式在SGD中，选择直接使用梯度下降的方法来更新参数  ，其更新公式如下：但是为了泛用性，这里通常使用样本  来代替真正的价值函数  例如  可能是带有噪声的版本或者直接采样取到的样本，基于蒙特卡洛的随机梯度下降流程图如下：
半梯度方法以  为学习目标，其更新公式是：半梯度学习方法减小了误差，在梯度下降的学习方法里面，本身的更新会受到weight的影响，导致算出来的不是真正的梯度。
线性方法线性方法就是使用线性函数来拟合价值函数。即定义：在使用线性函数的时候，其实可以不使用梯度下降的方法，因为这个时候可以采用最小二乘法求出精确的最优解。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/06/RL/RLBook2020%20Learning/Temporal-Difference%20Learning/" title="Temporal-Difference Learning">Temporal-Difference Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-06T10:24:25.000Z" title="Created 2024-03-06 18:24:25">2024-03-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">TD-预测在蒙特卡洛方法中，对于 every-visit 的蒙特卡洛方法，可以给出一个递推的更新公式：这个式子里面的  必须在一幕结束之后才能计算出来，所以在一幕中学不到任何信息。
而  的定义是：如果使用  来估算  的话，那么有带入有注意到这里的式子里面已经没有东西需要在一个幕结束之后才能算出来，那么就得到了一个只需要一步的时序差分方法，称为  或单步 
算法如图所示：
TD-Error定义 TD-error 为：那么式  可以写为  
蒙特卡洛误差可以被写成 TD-Error 的形式：
Sarsa方法前面给出了状态价值函数的更新公式，但是在实际做出决策的时候，动作价值函数是更为实用的选择，所以这里给出动作价值函数的更新公式：这里的公式中同样不含有任何需要一个episode结束才能算出来的东西，所以可以动态更新。
On-policy的时序差分方法在给定策略  的情况下，可以根据公式  来更新动作价值函数，并且更新  来逼近最优的策略。算法如图：
Q-Learning —— Off-Policy 的时序差分方法定义更新公式：Q-Learning已经证明是不依赖初始策略，以概率为1去逼 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/05/RL/RLBook2020%20Learning/Monte%20Carlo%20Methods/" title="Monte Carlo Methods">Monte Carlo Methods</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-05T06:03:58.000Z" title="Created 2024-03-05 14:03:58">2024-03-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">提出背景在DP方法中，需要指导转移概率的先验知识，但是一般情况夏是不知道这个概率的，即无法知晓  那么前面的 DP 方法就会失效，那么这里提出一种基于采样的方法来获取相关的信息。蒙特卡洛方法指的是一系列从重复采样中进行估计的方法。在强化学习中，蒙特卡洛方法通过对大量样本的回报进行平均，来进行价值估计。随着收集到的样本越来越多，平均值就会收敛到期望值。MC算法是在每一幕结束之后进行更新的。
关于 visit 访问的定义如果状态  出现在策略  中，每一次出现称为一次 访问 , 第一次访问称为 首次访问 
两种MC算法MC算法分为首次访问和每次访问两种，首次访问是只计算每一幕的首次访问的回报，而每次访问则是每一次遇到状态  就进行更新，首次访问的算法流程图如下：去掉关于  的判断就是每次访问的算法了。
利用蒙特卡洛方法估计动作价值如果问题是 Model-unaware 的那么就可以考虑利用蒙特卡洛方法去估计动作-状态价值函数 ，只需要把上面的算法中的  换成  二元组就可以了。
消除初始策略带来的影响为了尽可能多的探索可能的情况，减小策略带来的影响而学习到全局的最优解，通常有 on-pol ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/03/RL/RLBook2020%20Learning/Dynamic%20Programming/" title="Dynamic Programming">Dynamic Programming</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-03T07:07:35.000Z" title="Created 2024-03-03 15:07:35">2024-03-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">递推表达式通过之前的定义可以得到一个递推版本的DP状态转移方程：

这里的  代表的是  步，具体的含义是可以通过  次 action 到达这个状态。所以上面的更新就是从  步的价值函数去更新  步的价值函数这里的并不是Bellman 方程，只是递推表达式，算法要求是到最后接近满足Bellman方程


注意，这里的更新是和策略  有关的，是在策略确定的情况下，通过更新的方式来确定真正的状态价值函数。

具体的算法如图：
在递推的过程中改进策略在迭代的过程中，如果已知策略  的价值函数  希望知道在某个状态  下选择一个不同于  的动作  是否会带来改善，这种策略的价值为：如果上面的式子的值大于目前的状态价值函数  那么就更新此时的策略为  
而由于所有的策略的状态价值函数存在偏序关系，也就是说存在 upper bound 那么就可以利用这一点证明，每次取贪心的策略  即$$\pi’ = \mathop{argmax}a q_\pi(s,a) = \mathop{argmax}a \mathbb E[R{t + 1} + \gamma v_\pi (S{t +1}) | S_t = s ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/02/RL/RLBook2020%20Learning/Finite%20Markov%20Decision/" title="Finite Markov Decision">Finite Markov Decision</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-02T07:01:54.000Z" title="Created 2024-03-02 15:01:54">2024-03-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">马尔科夫模型中与环境交互的定义Agent做出动作  后，Environment会反馈一个状态  和一个奖励  给到Agent，而Agent的目标还是最大化奖励之和
有限马尔科夫决策过程的规定在有限马尔科夫决策过程中，所有的 states,actions,rewards 的集合都是有限的，而随机变量  和  被定义为仅仅依靠前面一次的state和action 的离散的概率分布，即只有上一次的状态和选择会影响当前的状态和奖励。
转移函数定义转移函数  :转移函数  是一个确定性的函数，即在同一个马尔科夫随机过程中，这个函数是不会发生变化的 
该函数有如下的性质：




奖励期望的定义在MDP中，奖励的期望被定义为
如何确定合理的奖励这里的奖励应该设置成为学习的额最终目标，例如如果是训练围棋，那么奖励应该设置为获得胜利，只有获得胜利的时候才会得到1的奖励，不能设置为吃子，这样训练的结果会变成一个以吃子为目标而不是以获胜为目标的算法。
两种不同的任务类型可以分成 episode 的如果 agent 与 environment 的交互可以自然地分成多个 episode 那么就可以得到一个终结状 ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/#content-inner">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/#content-inner">8</a><a class="extend next" rel="next" href="/page/5/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/OIP.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Eric Li</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">80</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">32</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ericli2333"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ericli2333" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">The blog is now under construction</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/07/IRL/Article%20Reading/PPO/" title="PPO">PPO</a><time datetime="2025-03-07T08:24:54.000Z" title="Created 2025-03-07 16:24:54">2025-03-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/17/Fla/FLA%20Lab%20Report/" title="FLA Lab Report | 自动机大作业实验报告">FLA Lab Report | 自动机大作业实验报告</a><time datetime="2025-01-17T03:06:22.000Z" title="Created 2025-01-17 11:06:22">2025-01-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/07/DataBase/Database%20Review/" title="Database Review | 数据库期末复习笔记">Database Review | 数据库期末复习笔记</a><time datetime="2025-01-07T02:10:27.000Z" title="Created 2025-01-07 10:10:27">2025-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/04/Blogs/Artificial%20Intelligence%20Review%20Note/" title="Artificial Intelligence Review Note | 人工智能复习笔记">Artificial Intelligence Review Note | 人工智能复习笔记</a><time datetime="2025-01-04T12:34:07.000Z" title="Created 2025-01-04 20:34:07">2025-01-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/23/DataBase/DB%20Takeaway%20Notes/" title="DB Takeaway Notes ｜ 易错点">DB Takeaway Notes ｜ 易错点</a><time datetime="2024-12-23T06:10:30.000Z" title="Created 2024-12-23 14:10:30">2024-12-23</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI/"><span class="card-category-list-name">AI</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Algorithm/"><span class="card-category-list-name">Algorithm</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Article-Reading/"><span class="card-category-list-name">Article_Reading</span><span class="card-category-list-count">1</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Article-Reading/reinforcement-learning/"><span class="card-category-list-name">reinforcement_learning</span><span class="card-category-list-count">1</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/CV/"><span class="card-category-list-name">CV</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cryptology/"><span class="card-category-list-name">Cryptology</span><span class="card-category-list-count">5</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DB/"><span class="card-category-list-name">DB</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Diffusion/"><span class="card-category-list-name">Diffusion</span><span class="card-category-list-count">1</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/InformationTheory/" style="font-size: 1.1em; color: #999">InformationTheory</a> <a href="/tags/AI/" style="font-size: 1.1em; color: #999">AI</a> <a href="/tags/notes/" style="font-size: 1.5em; color: #99a9bf">notes</a> <a href="/tags/termux/" style="font-size: 1.1em; color: #999">termux</a> <a href="/tags/RBT/" style="font-size: 1.1em; color: #999">RBT</a> <a href="/tags/CV/" style="font-size: 1.1em; color: #999">CV</a> <a href="/tags/LaTeX/" style="font-size: 1.1em; color: #999">LaTeX</a> <a href="/tags/Cryptology/" style="font-size: 1.37em; color: #99a4b2">Cryptology</a> <a href="/tags/wsl/" style="font-size: 1.1em; color: #999">wsl</a> <a href="/tags/IRL/" style="font-size: 1.3em; color: #99a1ac">IRL</a> <a href="/tags/hexo/" style="font-size: 1.1em; color: #999">hexo</a> <a href="/tags/frp/" style="font-size: 1.1em; color: #999">frp</a> <a href="/tags/fla/" style="font-size: 1.23em; color: #999ea6">fla</a> <a href="/tags/Optimization/" style="font-size: 1.43em; color: #99a6b9">Optimization</a> <a href="/tags/OS/" style="font-size: 1.17em; color: #999c9f">OS</a> <a href="/tags/Transformer/" style="font-size: 1.1em; color: #999">Transformer</a> <a href="/tags/AVL/" style="font-size: 1.1em; color: #999">AVL</a> <a href="/tags/Math/" style="font-size: 1.1em; color: #999">Math</a> <a href="/tags/certbot/" style="font-size: 1.1em; color: #999">certbot</a> <a href="/tags/Others/" style="font-size: 1.1em; color: #999">Others</a> <a href="/tags/acme/" style="font-size: 1.1em; color: #999">acme</a> <a href="/tags/ssl/" style="font-size: 1.17em; color: #999c9f">ssl</a> <a href="/tags/RegularExpression/" style="font-size: 1.1em; color: #999">RegularExpression</a> <a href="/tags/gitlab-ci/" style="font-size: 1.1em; color: #999">gitlab-ci</a> <a href="/tags/ICS/" style="font-size: 1.1em; color: #999">ICS</a> <a href="/tags/MySQL/" style="font-size: 1.1em; color: #999">MySQL</a> <a href="/tags/cmake/" style="font-size: 1.1em; color: #999">cmake</a> <a href="/tags/B%E6%A0%91/" style="font-size: 1.1em; color: #999">B树</a> <a href="/tags/latex/" style="font-size: 1.1em; color: #999">latex</a> <a href="/tags/m%E8%B7%AF%E6%90%9C%E7%B4%A2%E6%A0%91/" style="font-size: 1.1em; color: #999">m路搜索树</a> <a href="/tags/3D/" style="font-size: 1.1em; color: #999">3D</a> <a href="/tags/server/" style="font-size: 1.37em; color: #99a4b2">server</a> <a href="/tags/Diffusion/" style="font-size: 1.1em; color: #999">Diffusion</a> <a href="/tags/config/" style="font-size: 1.3em; color: #99a1ac">config</a> <a href="/tags/SQL/" style="font-size: 1.1em; color: #999">SQL</a> <a href="/tags/Network/" style="font-size: 1.23em; color: #999ea6">Network</a> <a href="/tags/DB/" style="font-size: 1.23em; color: #999ea6">DB</a> <a href="/tags/VSCode/" style="font-size: 1.1em; color: #999">VSCode</a> <a href="/tags/Algorithm/" style="font-size: 1.17em; color: #999c9f">Algorithm</a> <a href="/tags/texlive/" style="font-size: 1.1em; color: #999">texlive</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/03/"><span class="card-archive-list-date">March 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">January 2025</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">December 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">November 2024</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">October 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">September 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">August 2024</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">July 2024</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">80</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-03-07T11:32:39.864Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/fufu.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Eric Li</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><imgsrc="https://haiyong.site/img/icp.png"><a href="https://beian.miit.gov.cn/#/Integrated/index"  style="color:white" target="_blank">蜀ICP备2023025661号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init(["循此苦旅，以达繁星","Per Aspera Ad Astra"])
  } else {
    document.getElementById("subtitle").textContent = "循此苦旅，以达繁星"
  }
}
typedJSFn.run(subtitleType)</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>