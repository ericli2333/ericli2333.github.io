<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Blogs</title><meta name="author" content="Eric Li"><meta name="copyright" content="Eric Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Blogs">
<meta property="og:url" content="https://www.ericli.vip/page/4/index.html">
<meta property="og:site_name" content="Blogs">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.ericli.vip/img/OIP.jpg">
<meta property="article:author" content="Eric Li">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.ericli.vip/img/OIP.jpg"><link rel="shortcut icon" href="/img/OIP.jpg"><link rel="canonical" href="https://www.ericli.vip/page/4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Blogs',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-01-07 15:19:39'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Blogs" type="application/atom+xml">
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/OIP.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">77</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">31</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/fufu.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Blogs"><span class="site-name">Blogs</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Blogs</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/ericli2333" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/13/RL/RLBook2020%20Learning/Policy%20Gradient%20Methods/" title="Policy Gradient Methods">Policy Gradient Methods</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-13T08:43:39.000Z" title="Created 2024-03-13 16:43:39">2024-03-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Background本书前面的部分主要讲的都是学习价值函数的方法，这里提出一种直接学习策略的方法，这里把策略记作一个带有参数的，即  
Advantage优点之一是可以学习一个确定性的算法而不像  - greedy 的策略那样每次都有一个较小的概率选择非最优解。同时，基于价值函数学习的方法里面如何选择初始值和如何进行递降都是需要考虑的问题。
Policy Gradient在直接学习策略的时候，正确地更新参数  是十分重要的，所以需要想办法求出评估量对于  的梯度，此处定义：对于分幕式任务，在经过推导（RLBook2020 P325）后，得到：

这里虽然只找出了正比关系，但是在梯度下降的时候，只关注梯度的方向，并不关心梯度真正的值是多少

Monte Carlo Policy Gradient根据上面的式子，写成期望的形式：那么就可以得出梯度下降的公式：其中  是对于真实动作价值函数的逼近。上面的公式成为 all-actions methods 因为它包含了该状态所有可能的动作  下面介绍另一种强化学习版本的。
这里的  是依据策略  在  时刻采样出一个动作，第一个等号相等的原因是因 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/12/RL/RLBook2020%20Learning/On-policy%20Control%20with%20Approximation/" title="On-policy Control withApproximation">On-policy Control withApproximation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-12T13:06:40.000Z" title="Created 2024-03-12 21:06:40">2024-03-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Episodic Semi-gradient Control这里和上一章的公式的区别只是把状态价值函数改成了动作价值函数，即：对于一步的Sarsa算法来说，上面的公式应该写为：其算法流程图如下：而对于  步的Sarsa来说，和前面也没有太大的差别：只是这里的价值函数更新的时候变成了以n为周期的。
平均回报对于可以分为一个个Episode的任务，前面的折后回报是可以处理的，但是对于连续型任务是不够的，定义一个策略  的平均回报  如下：那么这个时候的状态价值函数、动作价值函数和最优状态价值函数和最优动作价值函数都可以写成一个新的形式：最优的就是取最大值就行了。

注意到上面的式子里面没有折扣率  了，因为在连续型任务中，先后出现的价值在重要性上没有区别。

在这种定义下的单步Sarsa算法如下：n步Sarsa的算法如下图所示：
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/10/RL/RLBook2020%20Learning/On-policy%20Prediction%20with%20Approximation/" title="On-policy Prediction with Approximation">On-policy Prediction with Approximation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-10T07:53:49.000Z" title="Created 2024-03-10 15:53:49">2024-03-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">提出背景由于某些问题的空间维度可能会很高，直接使用tabular的方法来保存所有信息是不现实的，所以考虑换一种方法来表示价值函数，即使用  来近似替代原来的状态价值函数
均方误差为了评估近似替代版本的价值函数和原始的价值函数之间的距离，这里提出均方误差  其定义为：
这其中的  是状态的分布，是状态  出现的概率
SGD和Semi-gradient MethodsSGD 的更新公式在SGD中，选择直接使用梯度下降的方法来更新参数  ，其更新公式如下：但是为了泛用性，这里通常使用样本  来代替真正的价值函数  例如  可能是带有噪声的版本或者直接采样取到的样本，基于蒙特卡洛的随机梯度下降流程图如下：
半梯度方法以  为学习目标，其更新公式是：半梯度学习方法减小了误差，在梯度下降的学习方法里面，本身的更新会受到weight的影响，导致算出来的不是真正的梯度。
线性方法线性方法就是使用线性函数来拟合价值函数。即定义：在使用线性函数的时候，其实可以不使用梯度下降的方法，因为这个时候可以采用最小二乘法求出精确的最优解。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/06/RL/RLBook2020%20Learning/Temporal-Difference%20Learning/" title="Temporal-Difference Learning">Temporal-Difference Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-06T10:24:25.000Z" title="Created 2024-03-06 18:24:25">2024-03-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">TD-预测在蒙特卡洛方法中，对于 every-visit 的蒙特卡洛方法，可以给出一个递推的更新公式：这个式子里面的  必须在一幕结束之后才能计算出来，所以在一幕中学不到任何信息。
而  的定义是：如果使用  来估算  的话，那么有带入有注意到这里的式子里面已经没有东西需要在一个幕结束之后才能算出来，那么就得到了一个只需要一步的时序差分方法，称为  或单步 
算法如图所示：
TD-Error定义 TD-error 为：那么式  可以写为  
蒙特卡洛误差可以被写成 TD-Error 的形式：
Sarsa方法前面给出了状态价值函数的更新公式，但是在实际做出决策的时候，动作价值函数是更为实用的选择，所以这里给出动作价值函数的更新公式：这里的公式中同样不含有任何需要一个episode结束才能算出来的东西，所以可以动态更新。
On-policy的时序差分方法在给定策略  的情况下，可以根据公式  来更新动作价值函数，并且更新  来逼近最优的策略。算法如图：
Q-Learning —— Off-Policy 的时序差分方法定义更新公式：Q-Learning已经证明是不依赖初始策略，以概率为1去逼 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/05/RL/RLBook2020%20Learning/Monte%20Carlo%20Methods/" title="Monte Carlo Methods">Monte Carlo Methods</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-05T06:03:58.000Z" title="Created 2024-03-05 14:03:58">2024-03-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">提出背景在DP方法中，需要指导转移概率的先验知识，但是一般情况夏是不知道这个概率的，即无法知晓  那么前面的 DP 方法就会失效，那么这里提出一种基于采样的方法来获取相关的信息。蒙特卡洛方法指的是一系列从重复采样中进行估计的方法。在强化学习中，蒙特卡洛方法通过对大量样本的回报进行平均，来进行价值估计。随着收集到的样本越来越多，平均值就会收敛到期望值。MC算法是在每一幕结束之后进行更新的。
关于 visit 访问的定义如果状态  出现在策略  中，每一次出现称为一次 访问 , 第一次访问称为 首次访问 
两种MC算法MC算法分为首次访问和每次访问两种，首次访问是只计算每一幕的首次访问的回报，而每次访问则是每一次遇到状态  就进行更新，首次访问的算法流程图如下：去掉关于  的判断就是每次访问的算法了。
利用蒙特卡洛方法估计动作价值如果问题是 Model-unaware 的那么就可以考虑利用蒙特卡洛方法去估计动作-状态价值函数 ，只需要把上面的算法中的  换成  二元组就可以了。
消除初始策略带来的影响为了尽可能多的探索可能的情况，减小策略带来的影响而学习到全局的最优解，通常有 on-pol ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/03/RL/RLBook2020%20Learning/Dynamic%20Programming/" title="Dynamic Programming">Dynamic Programming</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-03T07:07:35.000Z" title="Created 2024-03-03 15:07:35">2024-03-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">递推表达式通过之前的定义可以得到一个递推版本的DP状态转移方程：

这里的  代表的是  步，具体的含义是可以通过  次 action 到达这个状态。所以上面的更新就是从  步的价值函数去更新  步的价值函数这里的并不是Bellman 方程，只是递推表达式，算法要求是到最后接近满足Bellman方程


注意，这里的更新是和策略  有关的，是在策略确定的情况下，通过更新的方式来确定真正的状态价值函数。

具体的算法如图：
在递推的过程中改进策略在迭代的过程中，如果已知策略  的价值函数  希望知道在某个状态  下选择一个不同于  的动作  是否会带来改善，这种策略的价值为：如果上面的式子的值大于目前的状态价值函数  那么就更新此时的策略为  
而由于所有的策略的状态价值函数存在偏序关系，也就是说存在 upper bound 那么就可以利用这一点证明，每次取贪心的策略  即$$\pi’ = \mathop{argmax}a q_\pi(s,a) = \mathop{argmax}a \mathbb E[R{t + 1} + \gamma v_\pi (S{t +1}) | S_t = s ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/02/RL/RLBook2020%20Learning/Finite%20Markov%20Decision/" title="Finite Markov Decision">Finite Markov Decision</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-02T07:01:54.000Z" title="Created 2024-03-02 15:01:54">2024-03-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">马尔科夫模型中与环境交互的定义Agent做出动作  后，Environment会反馈一个状态  和一个奖励  给到Agent，而Agent的目标还是最大化奖励之和
有限马尔科夫决策过程的规定在有限马尔科夫决策过程中，所有的 states,actions,rewards 的集合都是有限的，而随机变量  和  被定义为仅仅依靠前面一次的state和action 的离散的概率分布，即只有上一次的状态和选择会影响当前的状态和奖励。
转移函数定义转移函数  :转移函数  是一个确定性的函数，即在同一个马尔科夫随机过程中，这个函数是不会发生变化的 
该函数有如下的性质：




奖励期望的定义在MDP中，奖励的期望被定义为
如何确定合理的奖励这里的奖励应该设置成为学习的额最终目标，例如如果是训练围棋，那么奖励应该设置为获得胜利，只有获得胜利的时候才会得到1的奖励，不能设置为吃子，这样训练的结果会变成一个以吃子为目标而不是以获胜为目标的算法。
两种不同的任务类型可以分成 episode 的如果 agent 与 environment 的交互可以自然地分成多个 episode 那么就可以得到一个终结状 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/28/RL/RLBook2020%20Learning/Tabular%20Solution%20Methods/" title="Tabular Solution Methods">Tabular Solution Methods</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-02-28T08:04:49.000Z" title="Created 2024-02-28 16:04:49">2024-02-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Basic idea基本想法是在状态空间和可能的行动空间都足够小的情况下（能够通过表格或者是数列存下的情况下），通常能够找到精确解
多臂老虎机(k-armed bandit problem)问题描述现在有 k 种选择，每次可以从 k 种选择种选择一个，每种选择给出的奖励都是基于一个未知但是确定的分布的，学习的目标是在有限的次数中最大化所有的奖励的和。
符号定义
 : 在  时刻选择的动作
 : 对应的reward 
 : 在该时刻动作  的 value：
 : 在时刻  估计的动作  的 value

样本均值法此时对于动作  的估计是：$$Q_t(a) = \frac{\mathrm{在t时刻之前采取动作a获得的奖励之和}}{在t时刻之前采取动作a的次数} = \frac{\sum_{i = 1}^{t - 1}\limits R_i \cdot \mathbb{1}{A_i = a}}{\sum{i = 1}^{t - 1}\limits \mathbb{1}{A_i = a}}$$这里的 $\mathbb{1}{A_i = a}$ 是指示函数，在相等时取1否则为0
那么每次的决策 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/02/28/RL/RLBook2020%20Learning/Introduction/" title="Introduction to reinforcement learning">Introduction to reinforcement learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-02-28T08:00:00.000Z" title="Created 2024-02-28 16:00:00">2024-02-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Introduction强化学习的基本思想是从与环境的互动中学习，与其他学习方式最大的两个区别就是：

trial-and-error search 
delayed reward

基本元素
policy
reward signal
value function
a model of environment

policy指agent每次在特定的时间下选择action的策略
reward signal指的是整个强化学习的目标，每一次做出决策之后，环境都会给予一个反馈，这里的reward signal是及时反馈
value function这里的value function是长期的反馈，是用于衡量一个决策的长期收益的。
value的定义是指未来获得的奖励(reward)的总和的期望。value是基于reward的，只有有reward才能衡量value
Modelmodel是用来模拟环境变化的，是用来做计划的，强化学习算法可以分为model-based和model-free的
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/11/16/Optimization/Optimization%20Problems/" title="Optimization Problems">Optimization Problems</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-11-16T06:12:32.000Z" title="Created 2023-11-16 14:12:32">2023-11-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Optimization/">Optimization</a></span></div><div class="content">SOCP问题![[image/Pasted image 20231116141755.png]]
Robust Linear Programming![[image/Pasted image 20231116141315.png]]问题：

这里有无穷个约束，并且不可明确写出到底是哪些约束
所以这个就不是一个线性规划问题了解决方法：
取出上确界即可![[image/Pasted image 20231116141551.png]]后面那个是由于同方向的时候整个内积取最大值最后可以化成一个SOCP问题![[image/Pasted image 20231116141659.png]]

Geometric ProgrammingMonomial Function（单项式）![[image/Pasted image 20231116141938.png]]
Posynomial Function（正项式）![[image/Pasted image 20231116142008.png]]
GP问题（几何优化问题）![[image/Pasted image 20231116142059.pn ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/#content-inner">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/#content-inner">8</a><a class="extend next" rel="next" href="/page/5/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/OIP.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Eric Li</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">77</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">31</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ericli2333"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ericli2333" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">The blog is now under construction</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/07/DataBase/Database%20Review/" title="Database Review | 数据库期末复习笔记">Database Review | 数据库期末复习笔记</a><time datetime="2025-01-07T02:10:27.000Z" title="Created 2025-01-07 10:10:27">2025-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/04/Blogs/Artificial%20Intelligence%20Review%20Note/" title="Artificial Intelligence Review Note | 人工智能复习笔记">Artificial Intelligence Review Note | 人工智能复习笔记</a><time datetime="2025-01-04T12:34:07.000Z" title="Created 2025-01-04 20:34:07">2025-01-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/22/Fla/Pumping%20Lemma/" title="Pumping Lemma | 各种泵引理">Pumping Lemma | 各种泵引理</a><time datetime="2024-12-22T02:33:43.000Z" title="Created 2024-12-22 10:33:43">2024-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/20/Fla/P%20and%20NP,%20Decidable%20and%20RE/" title="P and NP,Decidable and RE">P and NP,Decidable and RE</a><time datetime="2024-12-20T05:58:57.000Z" title="Created 2024-12-20 13:58:57">2024-12-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/20/Diffusion/DDPM/" title="DDPM ｜ Diffusion基础">DDPM ｜ Diffusion基础</a><time datetime="2024-11-20T06:59:25.000Z" title="Created 2024-11-20 14:59:25">2024-11-20</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI/"><span class="card-category-list-name">AI</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Algorithm/"><span class="card-category-list-name">Algorithm</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Article-Reading/"><span class="card-category-list-name">Article_Reading</span><span class="card-category-list-count">1</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Article-Reading/reinforcement-learning/"><span class="card-category-list-name">reinforcement_learning</span><span class="card-category-list-count">1</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/CV/"><span class="card-category-list-name">CV</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cryptology/"><span class="card-category-list-name">Cryptology</span><span class="card-category-list-count">5</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DB/"><span class="card-category-list-name">DB</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Diffusion/"><span class="card-category-list-name">Diffusion</span><span class="card-category-list-count">1</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/gitlab/" style="font-size: 1.17em; color: #999c9f">gitlab</a> <a href="/tags/RegularExpression/" style="font-size: 1.1em; color: #999">RegularExpression</a> <a href="/tags/3D/" style="font-size: 1.1em; color: #999">3D</a> <a href="/tags/CV/" style="font-size: 1.1em; color: #999">CV</a> <a href="/tags/SQL/" style="font-size: 1.1em; color: #999">SQL</a> <a href="/tags/hexo/" style="font-size: 1.1em; color: #999">hexo</a> <a href="/tags/ICS/" style="font-size: 1.1em; color: #999">ICS</a> <a href="/tags/MySQL/" style="font-size: 1.1em; color: #999">MySQL</a> <a href="/tags/ssl/" style="font-size: 1.17em; color: #999c9f">ssl</a> <a href="/tags/frp/" style="font-size: 1.1em; color: #999">frp</a> <a href="/tags/notes/" style="font-size: 1.5em; color: #99a9bf">notes</a> <a href="/tags/LaTeX/" style="font-size: 1.1em; color: #999">LaTeX</a> <a href="/tags/Transformer/" style="font-size: 1.1em; color: #999">Transformer</a> <a href="/tags/texlive/" style="font-size: 1.1em; color: #999">texlive</a> <a href="/tags/Math/" style="font-size: 1.1em; color: #999">Math</a> <a href="/tags/VSCode/" style="font-size: 1.1em; color: #999">VSCode</a> <a href="/tags/gitlab-ci/" style="font-size: 1.1em; color: #999">gitlab-ci</a> <a href="/tags/AVL/" style="font-size: 1.1em; color: #999">AVL</a> <a href="/tags/fla/" style="font-size: 1.17em; color: #999c9f">fla</a> <a href="/tags/cpp/" style="font-size: 1.1em; color: #999">cpp</a> <a href="/tags/cmake/" style="font-size: 1.1em; color: #999">cmake</a> <a href="/tags/OS/" style="font-size: 1.17em; color: #999c9f">OS</a> <a href="/tags/RBT/" style="font-size: 1.1em; color: #999">RBT</a> <a href="/tags/Probability-Theory/" style="font-size: 1.17em; color: #999c9f">Probability_Theory</a> <a href="/tags/server/" style="font-size: 1.37em; color: #99a4b2">server</a> <a href="/tags/AI/" style="font-size: 1.1em; color: #999">AI</a> <a href="/tags/Integral/" style="font-size: 1.1em; color: #999">Integral</a> <a href="/tags/m%E8%B7%AF%E6%90%9C%E7%B4%A2%E6%A0%91/" style="font-size: 1.1em; color: #999">m路搜索树</a> <a href="/tags/git/" style="font-size: 1.1em; color: #999">git</a> <a href="/tags/latex/" style="font-size: 1.1em; color: #999">latex</a> <a href="/tags/InformationTheory/" style="font-size: 1.1em; color: #999">InformationTheory</a> <a href="/tags/certbot/" style="font-size: 1.1em; color: #999">certbot</a> <a href="/tags/acme/" style="font-size: 1.1em; color: #999">acme</a> <a href="/tags/DB/" style="font-size: 1.17em; color: #999c9f">DB</a> <a href="/tags/Algorithm/" style="font-size: 1.17em; color: #999c9f">Algorithm</a> <a href="/tags/RL/" style="font-size: 1.43em; color: #99a6b9">RL</a> <a href="/tags/Others/" style="font-size: 1.1em; color: #999">Others</a> <a href="/tags/Cryptology/" style="font-size: 1.37em; color: #99a4b2">Cryptology</a> <a href="/tags/config/" style="font-size: 1.3em; color: #99a1ac">config</a> <a href="/tags/Network/" style="font-size: 1.23em; color: #999ea6">Network</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">January 2025</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">December 2024</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">November 2024</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">October 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">September 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">August 2024</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">July 2024</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">June 2024</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">77</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-01-07T07:19:39.081Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/fufu.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Eric Li</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><imgsrc="https://haiyong.site/img/icp.png"><a href="https://beian.miit.gov.cn/#/Integrated/index"  style="color:white" target="_blank">蜀ICP备2023025661号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init(["循此苦旅，以达繁星","Per Aspera Ad Astra"])
  } else {
    document.getElementById("subtitle").textContent = "循此苦旅，以达繁星"
  }
}
typedJSFn.run(subtitleType)</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>