<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Blogs</title><meta name="author" content="Eric Li"><meta name="copyright" content="Eric Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Blogs">
<meta property="og:url" content="https://www.ericli.vip/page/5/index.html">
<meta property="og:site_name" content="Blogs">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.ericli.vip/img/OIP.jpg">
<meta property="article:author" content="Eric Li">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.ericli.vip/img/OIP.jpg"><link rel="shortcut icon" href="/img/OIP.jpg"><link rel="canonical" href="https://www.ericli.vip/page/5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Blogs',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-10-26 22:42:43'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Blogs" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/OIP.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">93</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">51</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">34</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/fufu.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Blogs"><span class="site-name">Blogs</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Blogs</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/ericli2333" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/04/30/Bachelor/ICS%20and%20OS/Concurrenct%20Programming/" title="Concurrent Programming —— Thoughts about locks">Concurrent Programming —— Thoughts about locks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-04-30T06:30:56.000Z" title="Created 2024-04-30 14:30:56">2024-04-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/OS/">OS</a></span></div><div class="content">关于并发中锁、条件变量和信号量的一些随想锁——原子性的开始众所周知，在并发中，锁是最简单的并发源语，也是最简单的消除数据竞争维护原子性的方法，你甚至只需要使用一对语句：
123pthread_mutex_lock(&amp;lock);//CSpthread_mutex_unlock(&amp;lock);
就可以维护中间的代码的原子性了。
但是如果是朴素的自旋锁的实现会带来一些问题：

如果多个进程同时尝试抢占一把锁，那么会让多个CPU进入自旋状态，浪费CPU资源
某些线程被阻塞，继续运行的条件可能需要其它很多线程共同达成，只使用自旋锁这个CPU在很长时间内都不能得到利用那么如何解决第一个问题呢？

最朴素的思路就是，先尝试获取这个锁，如果获取成功了，就直接进入临界区，如果不成功，就sleep 一段时间，这段时间甚至可以是在不超过上限的情况下指数递增的，如果超过上限就重置为1。这个自旋锁的改进版本在某种意义上可以提高CPU的运行效率，但是仍然是不够的。比如某个线程有明确的唤醒条件，那么可不可以直接让他睡到这个条件达成再唤醒呢？
答案是可以的，我们可以引入条件变量这个并发源语
条件变量 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/04/28/RL%20and%20IRL/RL/Article%20Reading/DDPG/" title="DDPG">DDPG</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-04-28T08:41:24.000Z" title="Created 2024-04-28 16:41:24">2024-04-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Article-Reading/">Article_Reading</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/Article-Reading/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Basic Terminology评估策略的方法在使用折扣因子的基础上，记为策略  的性能指标
关于  的不严谨的估计首先，对于某一个状态  被转移到这个状态的概率是那么对于  就可以做出一个不严谨的估量，即：
策略梯度定理策略梯度定理阐释了如何计算策略的梯度，即：注意这里求梯度的时候是没有考虑  这个部分贡献的梯度的，具体的原因在我的另一篇文章Policy Gradient Methods中有提到，因为这里只关心梯度的方向，并不关心梯度的大小。
Actor-Critic 模式在上面的评估函数中，可以发现，想要统计出  并不是一件很简单的事情，如果使用蒙特卡罗方法采样来获取的话，会需要大量的时间来采集数据，这是不希望看到的。所以我们选择再造一个网络，来拟合  函数，以对这个策略作出评估。Actor 和Critic的分工如下：

Actor:负责根据策略梯度定理调整参数  的值来更新策略
Critic:学习一个Q的函数来拟合当前的价值函数，即拟合 此时更新的公式变成了：

Off-policy Actor-critic模式如果此时的行动策略  不等于当前的评估策略，即  在这个情况下，策 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/04/13/RL%20and%20IRL/RL/Article%20Reading/DQN%E6%94%B9%E8%BF%9B/" title="DQN改进">DQN改进</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-04-13T08:21:54.000Z" title="Created 2024-04-13 16:21:54">2024-04-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Article Reading本次阅读三篇论文，分别是 1509.06461 1511.05952 1511.06581，分别是优先经验回放、Double DQN 和 Dual DQN
Double DQNDouble DQN主要解决的问题是在贪心策略做最大化的时候出现的最大化偏差问题，什么是最大化偏差呢？
最大化偏差（过估计）正常情况下理想的最优策略下的状态价值函数和动作价值函数应该满足：即使这里的估计是无偏的，即满足也不代表它的二阶矩就是0（证明是把上面那个式子里面的正的部分和负的部分分开再求平方），假设在这种情况下，会有：（平均值原理）这个下界是紧下界。
最大化偏差问题会导致很多模型的训练出现训练不稳定或者就训练不起来的情况。
Double DQN如何解决最大化偏差问题Double DQN选择使用两个不同的网络来进行学习。首先是有一个 Target 网络用于对当前的动作进行评估，再用一个 Q-Network 来学习当前的价值。每过一段时间就用 Q网络来更新target网络。
这个时候的更新表达式如下：
$$T_t^{\mathrm{DoubleQ}} \equiv R_{t +1 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/21/RL%20and%20IRL/RL/Learning%20Notes/Tmux%20usage/" title="Tmux 使用简介">Tmux 使用简介</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-21T11:59:53.000Z" title="Created 2024-03-21 19:59:53">2024-03-21</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%95%99%E7%A8%8B/">教程</a></span></div><div class="content">tmux简介tmux是链接服务器跑服务的神器，可以在取消链接之后继续运行想要运行的程序
使用流程安装tmux使用
1sudo apt install tmux
即可
新建窗口1tmux new -s NAME
即可创建一个名为name的session，然后在里面运行你的指令即可
然后就可以直接关掉这个链接了
退出窗口如果想要退出当前的tmux session 可以先按下 ctrl + B 然后松开（这个时候没有变化是正常的）然后按下 D 就可以在不终止当前任务的情况下退出了。如果想直接终止这个任务，可以按下 ctrl + B + D 即不松手就行了。 
关闭session使用命令
1tmux ls
查看当前在运行的session，使用
1tmux kill-sesion -t NAME
关掉session就可以了
恢复session使用命令
1tmux a -t NAME
可以恢复一个session
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/20/RL%20and%20IRL/RL/Learning%20Notes/Environment%20Configuration/" title="Gymnasium Environment Configuration">Gymnasium Environment Configuration</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-20T09:07:42.000Z" title="Created 2024-03-20 17:07:42">2024-03-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">强化学习环境——gymnasium配置注意，现在已经是2024年了，建议使用最新的gymnasium而不是gym
配置正确的python版本现在是2024年的3月20日，目前的gymnasium不支持python3.12，建议使用conda创建一个3.11的环境：
1conda create -n RL python=3.11
然后进入这个环境中：
1conda activate RL
如果使用的是Windows下的powershell，此时你的终端最前面没有显示例如：
1(RL) xxx@xxx.xxx.xxx.xxx:~

而是：
1xxx@xxx.xxx.xxx.xxx:~

的话，建议先运行：
1conda init 
然后使用
1conda info
查看一下现在的环境是不是激活成功了
安装gymnasium这里有两个坑，第一个是直接安装 gymnasium 只是装了个白板，里面啥也没有，需要安装的是 gymnasium[atari] 和 gymnasium[accept-rom-license]记住，两个都要装
第二个坑是不知道为什么用conda install没有效果，所 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/18/RL%20and%20IRL/RL/Article%20Reading/DQN/" title="DQN">DQN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-18T14:25:06.000Z" title="Created 2024-03-18 22:25:06">2024-03-18</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/Article-Reading/">Article_Reading</a></span></div><div class="content">Article Reading本次DQN我选择阅读的是1312.5602这篇论文
Motivation
过往的方法在处理高维度的输入，例如视频和音频的时候略显乏力，往往依赖人工选取特征，但是可以通过卷积神经网络、多层感知机等方式直接利用神经网络来提取高维的特征。
深度学习里面往往需要大量的有标签的样本，但是RL有延迟奖励问题，一个动作的价值可能需要一个Episode结束之后才能确定
深度学习里面有一个重要假设是独立同分布采样，但是RL里面的数据往往是有很高的相关性的，不符合该假设

Idea如何处理图像输入前面套一个CNN对图像进行卷积处理，提取图像的特征进行有效的降维处理
如何处理样本量少的问题使用经验回放数组的方法，即创建一个buffer，每次获得一个状态-动作-价值-下一状态组的时候，不仅是运用这个组来进行训练，更是把它放到buffer里面，每次训练的时候从里面采样出一个batch，利用batch来进行训练
Limitation单神经网络训练不稳定在13年的原版论文里面，使用的是一个神经网络，没有分为target网络和Q网络，导致在训练的时候loss上下波动比较大，reward上 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/13/RL%20and%20IRL/RL/RLBook2020%20Learning/Policy%20Gradient%20Methods/" title="Policy Gradient Methods">Policy Gradient Methods</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-13T08:43:39.000Z" title="Created 2024-03-13 16:43:39">2024-03-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Background本书前面的部分主要讲的都是学习价值函数的方法，这里提出一种直接学习策略的方法，这里把策略记作一个带有参数的，即  
Advantage优点之一是可以学习一个确定性的算法而不像  - greedy 的策略那样每次都有一个较小的概率选择非最优解。同时，基于价值函数学习的方法里面如何选择初始值和如何进行递降都是需要考虑的问题。
Policy Gradient在直接学习策略的时候，正确地更新参数  是十分重要的，所以需要想办法求出评估量对于  的梯度，此处定义：对于分幕式任务，在经过推导（RLBook2020 P325）后，得到：

这里虽然只找出了正比关系，但是在梯度下降的时候，只关注梯度的方向，并不关心梯度真正的值是多少

Monte Carlo Policy Gradient根据上面的式子，写成期望的形式：那么就可以得出梯度下降的公式：其中  是对于真实动作价值函数的逼近。上面的公式成为 all-actions methods 因为它包含了该状态所有可能的动作  下面介绍另一种强化学习版本的。
这里的  是依据策略  在  时刻采样出一个动作，第一个等号相等的原因是因 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/12/RL%20and%20IRL/RL/RLBook2020%20Learning/On-policy%20Control%20with%20Approximation/" title="On-policy Control withApproximation">On-policy Control withApproximation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-12T13:06:40.000Z" title="Created 2024-03-12 21:06:40">2024-03-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">Episodic Semi-gradient Control这里和上一章的公式的区别只是把状态价值函数改成了动作价值函数，即：对于一步的Sarsa算法来说，上面的公式应该写为：其算法流程图如下：而对于  步的Sarsa来说，和前面也没有太大的差别：只是这里的价值函数更新的时候变成了以n为周期的。
平均回报对于可以分为一个个Episode的任务，前面的折后回报是可以处理的，但是对于连续型任务是不够的，定义一个策略  的平均回报  如下：那么这个时候的状态价值函数、动作价值函数和最优状态价值函数和最优动作价值函数都可以写成一个新的形式：最优的就是取最大值就行了。

注意到上面的式子里面没有折扣率  了，因为在连续型任务中，先后出现的价值在重要性上没有区别。

在这种定义下的单步Sarsa算法如下：n步Sarsa的算法如下图所示：
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/10/RL%20and%20IRL/RL/RLBook2020%20Learning/On-policy%20Prediction%20with%20Approximation/" title="On-policy Prediction with Approximation">On-policy Prediction with Approximation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-10T07:53:49.000Z" title="Created 2024-03-10 15:53:49">2024-03-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">提出背景由于某些问题的空间维度可能会很高，直接使用tabular的方法来保存所有信息是不现实的，所以考虑换一种方法来表示价值函数，即使用  来近似替代原来的状态价值函数
均方误差为了评估近似替代版本的价值函数和原始的价值函数之间的距离，这里提出均方误差  其定义为：
这其中的  是状态的分布，是状态  出现的概率
SGD和Semi-gradient MethodsSGD 的更新公式在SGD中，选择直接使用梯度下降的方法来更新参数  ，其更新公式如下：但是为了泛用性，这里通常使用样本  来代替真正的价值函数  例如  可能是带有噪声的版本或者直接采样取到的样本，基于蒙特卡洛的随机梯度下降流程图如下：
半梯度方法以  为学习目标，其更新公式是：半梯度学习方法减小了误差，在梯度下降的学习方法里面，本身的更新会受到weight的影响，导致算出来的不是真正的梯度。
线性方法线性方法就是使用线性函数来拟合价值函数。即定义：在使用线性函数的时候，其实可以不使用梯度下降的方法，因为这个时候可以采用最小二乘法求出精确的最优解。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/03/06/RL%20and%20IRL/RL/RLBook2020%20Learning/Temporal-Difference%20Learning/" title="Temporal-Difference Learning">Temporal-Difference Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-03-06T10:24:25.000Z" title="Created 2024-03-06 18:24:25">2024-03-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/reinforcement-learning/">reinforcement_learning</a></span></div><div class="content">TD-预测在蒙特卡洛方法中，对于 every-visit 的蒙特卡洛方法，可以给出一个递推的更新公式：这个式子里面的  必须在一幕结束之后才能计算出来，所以在一幕中学不到任何信息。
而  的定义是：如果使用  来估算  的话，那么有带入有注意到这里的式子里面已经没有东西需要在一个幕结束之后才能算出来，那么就得到了一个只需要一步的时序差分方法，称为  或单步 
算法如图所示：
TD-Error定义 TD-error 为：那么式  可以写为  
蒙特卡洛误差可以被写成 TD-Error 的形式：
Sarsa方法前面给出了状态价值函数的更新公式，但是在实际做出决策的时候，动作价值函数是更为实用的选择，所以这里给出动作价值函数的更新公式：这里的公式中同样不含有任何需要一个episode结束才能算出来的东西，所以可以动态更新。
On-policy的时序差分方法在给定策略  的情况下，可以根据公式  来更新动作价值函数，并且更新  来逼近最优的策略。算法如图：
Q-Learning —— Off-Policy 的时序差分方法定义更新公式：Q-Learning已经证明是不依赖初始策略，以概率为1去逼 ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/4/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/#content-inner">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/#content-inner">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/#content-inner">10</a><a class="extend next" rel="next" href="/page/6/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/OIP.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Eric Li</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">93</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">51</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">34</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ericli2333"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ericli2333" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">The blog is now under construction</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/26/Welcome/" title="No title">No title</a><time datetime="2025-10-26T14:38:28.142Z" title="Created 2025-10-26 22:38:28">2025-10-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/26/tools/PythonPackages/" title="Python Package Managers">Python Package Managers</a><time datetime="2025-10-26T13:28:18.000Z" title="Created 2025-10-26 21:28:18">2025-10-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/05/tools/Blogs/Hydra%20Learning/" title="Hydra | python参数配置包">Hydra | python参数配置包</a><time datetime="2025-06-05T08:06:02.000Z" title="Created 2025-06-05 16:06:02">2025-06-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/23/Article%20Reading%20Notes/Learning%20Combinatorial%20Optimization%20Algorithms%20over%20Graphs/" title="arXiv:1704.01665v4 | Learning Combinatorial Optimization Algorithms over Graphs | NCO论文阅读">arXiv:1704.01665v4 | Learning Combinatorial Optimization Algorithms over Graphs | NCO论文阅读</a><time datetime="2025-05-23T01:48:07.000Z" title="Created 2025-05-23 09:48:07">2025-05-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/24/RL%20and%20IRL/RL/Article%20Reading/GAE/" title="GAE | 广义优势估计">GAE | 广义优势估计</a><time datetime="2025-03-24T08:23:30.000Z" title="Created 2025-03-24 16:23:30">2025-03-24</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI/"><span class="card-category-list-name">AI</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Algorithm/"><span class="card-category-list-name">Algorithm</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Article-Reading/"><span class="card-category-list-name">Article_Reading</span><span class="card-category-list-count">1</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Article-Reading/reinforcement-learning/"><span class="card-category-list-name">reinforcement_learning</span><span class="card-category-list-count">1</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/CV/"><span class="card-category-list-name">CV</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cryptology/"><span class="card-category-list-name">Cryptology</span><span class="card-category-list-count">5</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DB/"><span class="card-category-list-name">DB</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Diffusion/"><span class="card-category-list-name">Diffusion</span><span class="card-category-list-count">1</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/IRL/" style="font-size: 1.27em; color: #99a0a9">IRL</a> <a href="/tags/termux/" style="font-size: 1.1em; color: #999">termux</a> <a href="/tags/RegularExpression/" style="font-size: 1.1em; color: #999">RegularExpression</a> <a href="/tags/Diffusion/" style="font-size: 1.1em; color: #999">Diffusion</a> <a href="/tags/wsl/" style="font-size: 1.1em; color: #999">wsl</a> <a href="/tags/Integral/" style="font-size: 1.1em; color: #999">Integral</a> <a href="/tags/CV/" style="font-size: 1.1em; color: #999">CV</a> <a href="/tags/ssl/" style="font-size: 1.16em; color: #999b9e">ssl</a> <a href="/tags/texlive/" style="font-size: 1.1em; color: #999">texlive</a> <a href="/tags/AVL/" style="font-size: 1.1em; color: #999">AVL</a> <a href="/tags/Probability-Theory/" style="font-size: 1.16em; color: #999b9e">Probability_Theory</a> <a href="/tags/gitlab-ci/" style="font-size: 1.1em; color: #999">gitlab-ci</a> <a href="/tags/latex/" style="font-size: 1.1em; color: #999">latex</a> <a href="/tags/Hydra/" style="font-size: 1.1em; color: #999">Hydra</a> <a href="/tags/config/" style="font-size: 1.27em; color: #99a0a9">config</a> <a href="/tags/Python/" style="font-size: 1.1em; color: #999">Python</a> <a href="/tags/Cryptology/" style="font-size: 1.33em; color: #99a2af">Cryptology</a> <a href="/tags/server/" style="font-size: 1.33em; color: #99a2af">server</a> <a href="/tags/OS/" style="font-size: 1.16em; color: #999b9e">OS</a> <a href="/tags/notes/" style="font-size: 1.5em; color: #99a9bf">notes</a> <a href="/tags/B%E6%A0%91/" style="font-size: 1.1em; color: #999">B树</a> <a href="/tags/Conda/" style="font-size: 1.1em; color: #999">Conda</a> <a href="/tags/MySQL/" style="font-size: 1.1em; color: #999">MySQL</a> <a href="/tags/InformationTheory/" style="font-size: 1.1em; color: #999">InformationTheory</a> <a href="/tags/Others/" style="font-size: 1.1em; color: #999">Others</a> <a href="/tags/acme/" style="font-size: 1.1em; color: #999">acme</a> <a href="/tags/cmake/" style="font-size: 1.1em; color: #999">cmake</a> <a href="/tags/VSCode/" style="font-size: 1.1em; color: #999">VSCode</a> <a href="/tags/DB/" style="font-size: 1.21em; color: #999ea4">DB</a> <a href="/tags/Math/" style="font-size: 1.1em; color: #999">Math</a> <a href="/tags/LaTeX/" style="font-size: 1.1em; color: #999">LaTeX</a> <a href="/tags/gitlab/" style="font-size: 1.16em; color: #999b9e">gitlab</a> <a href="/tags/3D/" style="font-size: 1.1em; color: #999">3D</a> <a href="/tags/Optimization/" style="font-size: 1.39em; color: #99a4b4">Optimization</a> <a href="/tags/Network/" style="font-size: 1.44em; color: #99a7ba">Network</a> <a href="/tags/fla/" style="font-size: 1.21em; color: #999ea4">fla</a> <a href="/tags/Transformer/" style="font-size: 1.1em; color: #999">Transformer</a> <a href="/tags/m%E8%B7%AF%E6%90%9C%E7%B4%A2%E6%A0%91/" style="font-size: 1.1em; color: #999">m路搜索树</a> <a href="/tags/Article/" style="font-size: 1.1em; color: #999">Article</a> <a href="/tags/UV/" style="font-size: 1.1em; color: #999">UV</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/10/"><span class="card-archive-list-date">October 2025</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/06/"><span class="card-archive-list-date">June 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">May 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/03/"><span class="card-archive-list-date">March 2025</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">January 2025</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">December 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">November 2024</span><span class="card-archive-list-count">9</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">October 2024</span><span class="card-archive-list-count">8</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">93</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-10-26T14:42:42.803Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/fufu.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Eric Li</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><imgsrc="https://haiyong.site/img/icp.png"><a href="https://beian.miit.gov.cn/#/Integrated/index"  style="color:white" target="_blank">蜀ICP备2023025661号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init(["循此苦旅，以达繁星","Per Aspera Ad Astra"])
  } else {
    document.getElementById("subtitle").textContent = "循此苦旅，以达繁星"
  }
}
typedJSFn.run(subtitleType)</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>